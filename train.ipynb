{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-yuki090626/yamashita/ErnieASR/scripts/ernie_asr\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter-yuki090626/yamashita/ErnieASR/scripts/ernie_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import kaldiio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from pathlib import PurePath\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.io import wavfile\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ読み込み(CSJ/core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trainset\n",
    "data = []\n",
    "with open('./data/train/csj_core_train.json', 'r') as f:\n",
    "    data = [json.loads(d) for d in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phones_list = [\"\", \"BOS\", \"EOS\", \"PAD\"]\n",
    "for d in data:\n",
    "    for phone in d[\"phones\"]:\n",
    "        if phone not in phones_list:\n",
    "            phones_list.append(phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phones_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'BOS',\n",
       " 'EOS',\n",
       " 'PAD',\n",
       " 'e',\n",
       " ':',\n",
       " 'q',\n",
       " 't',\n",
       " 'o',\n",
       " 'ky',\n",
       " 'r',\n",
       " 'i',\n",
       " 'ts',\n",
       " 'u',\n",
       " 'd',\n",
       " 'a',\n",
       " 'g',\n",
       " 'k',\n",
       " 'n',\n",
       " 'm',\n",
       " 'sh',\n",
       " 's',\n",
       " 'z',\n",
       " 'w',\n",
       " 'b',\n",
       " 'N',\n",
       " 'h',\n",
       " 'py',\n",
       " 'j',\n",
       " 'ch',\n",
       " 'ry',\n",
       " 'y',\n",
       " 'hy',\n",
       " 'p',\n",
       " 'f',\n",
       " 'my',\n",
       " 'ny',\n",
       " 'by',\n",
       " 'gy',\n",
       " 'dy',\n",
       " 'ty']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phones_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelEncoder = LabelEncoder()\n",
    "labelEncoder.fit(list(phones_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wavデータ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/train’: File exists\n"
     ]
    }
   ],
   "source": [
    "# !rm -rf data/train\n",
    "!mkdir data/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_data = glob.glob(\"data/csj-data/core/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym2path = {}\n",
    "\n",
    "for dir_path in core_data:\n",
    "    if dir_path == 'data/csj-data/core/$MYVIMRC':\n",
    "        continue\n",
    "    wav_symbol = dir_path.split('/')[-1]\n",
    "    with open(dir_path+\"/\"+wav_symbol+\"-wav.list\", \"r\") as f:\n",
    "        wavlist = f.readlines()\n",
    "    f.close()\n",
    "    wav_path = wavlist[0].strip()\n",
    "    sym2path[wav_symbol] = wav_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavscpf = open(\"data/train/wav.scp\", \"w\")\n",
    "utt2spkf = open(\"data/train/utt2spk\", \"w\")\n",
    "\n",
    "for d in data:\n",
    "    start, _ , duration = d[\"utt2info\"]\n",
    "    wavscp = d['uttname'] + ' sox ' + sym2path[d['uttname'].split('-')[0]] + ' -t wav -r 16000 -c 1 - trim {} {}|'.format(start, round(duration,3))\n",
    "    utt2spk = d['uttname'] + \" \" + d['uttname'].split('-')[0]\n",
    "    \n",
    "    wavscpf.write(wavscp + \"\\n\")\n",
    "    utt2spkf.write(utt2spk + \"\\n\")\n",
    "\n",
    "wavscpf.close()\n",
    "utt2spkf.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A11M0846-000443 sox /disk107/DATA/CSJ_RAW/WAV/core/A11M0846.wav -t wav -r 16000 -c 1 - trim 855.126 1.321|\n",
      "A11M0846-000444 sox /disk107/DATA/CSJ_RAW/WAV/core/A11M0846.wav -t wav -r 16000 -c 1 - trim 856.747 0.937|\n",
      "A11M0846-000445 sox /disk107/DATA/CSJ_RAW/WAV/core/A11M0846.wav -t wav -r 16000 -c 1 - trim 857.961 2.719|\n",
      "A11M0846-000446 sox /disk107/DATA/CSJ_RAW/WAV/core/A11M0846.wav -t wav -r 16000 -c 1 - trim 861.759 0.892|\n",
      "A11M0846-000447 sox /disk107/DATA/CSJ_RAW/WAV/core/A11M0846.wav -t wav -r 16000 -c 1 - trim 863.092 0.428|\n",
      "A11M0846-000448 sox /disk107/DATA/CSJ_RAW/WAV/core/A11M0846.wav -t wav -r 16000 -c 1 - trim 863.782 1.202|\n",
      "A11M0846-000449 sox /disk107/DATA/CSJ_RAW/WAV/core/A11M0846.wav -t wav -r 16000 -c 1 - trim 865.335 0.676|\n",
      "A11M0846-000450 sox /disk107/DATA/CSJ_RAW/WAV/core/A11M0846.wav -t wav -r 16000 -c 1 - trim 868.068 1.97|\n",
      "A11M0846-000451 sox /disk107/DATA/CSJ_RAW/WAV/core/A11M0846.wav -t wav -r 16000 -c 1 - trim 870.57 3.323|\n",
      "A11M0846-000452 sox /disk107/DATA/CSJ_RAW/WAV/core/A11M0846.wav -t wav -r 16000 -c 1 - trim 874.362 2.604|\n"
     ]
    }
   ],
   "source": [
    "!tail data/train/wav.scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFCC関連"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils/fix_data_dir.sh: file data/train/utt2spk is not in sorted order or not unique, sorting it\n",
      "utils/fix_data_dir.sh: file data/train/spk2utt is not in sorted order or not unique, sorting it\n",
      "utils/fix_data_dir.sh: file data/train/wav.scp is not in sorted order or not unique, sorting it\n",
      "fix_data_dir.sh: kept all 51675 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/train/.backup\n",
      "steps/make_mfcc.sh --write-utt2num-frames true --mfcc-config conf/mfcc.conf --nj 32 --cmd utils/run.pl data/train data/log data/mfcc\n",
      "utils/validate_data_dir.sh: Successfully validated data-directory data/train\n",
      "steps/make_mfcc.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.\n",
      "steps/make_mfcc.sh: Succeeded creating MFCC features for train\n",
      "steps/compute_cmvn_stats.sh data/train data/log/cmvnlog data/mfcc\n",
      "Succeeded creating CMVN stats for train\n",
      "fix_data_dir.sh: kept all 51675 utterances.\n",
      "fix_data_dir.sh: old files are kept in data/train/.backup\n"
     ]
    }
   ],
   "source": [
    "!bash run_mfcc.sh data/train data/mfcc data/log conf/mfcc.conf 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train/feats.scp') as f:\n",
    "    lines = f.readlines()\n",
    "f.close()\n",
    "utt2feats = {}\n",
    "for i in lines:\n",
    "    i = i.strip()\n",
    "    utt, feat_path = i.split(' ')\n",
    "    utt2feats[utt] = feat_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data, utt2feats, phones_list, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = data\n",
    "        self.data_num = len(data)\n",
    "        self.utt2feats = utt2feats\n",
    "        self.phones_list = phones_list\n",
    "        self.phonel = len(self.phones_list)\n",
    "        self.seq_length = 256\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_num\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        utt = self.data[idx]['uttname']\n",
    "        phones_label = self.data[idx][\"phones\"]\n",
    "        \n",
    "        feats = torch.tensor(kaldiio.load_mat(utt2feats[utt]))\n",
    "        label = torch.tensor(labelEncoder.transform(phones_label))\n",
    "        \n",
    "        return feats, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(data, utt2feats, phones_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1, test1 = dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([359, 30])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \"\"\"RNN module\n",
    "    :param int idim: dimension of inputs\n",
    "    :param int elayers: number of encoder layers\n",
    "    :param int cdim: number of rnn units (resulted in cdim * 2 if bidirectional)\n",
    "    :param int hdim: number of final projection units\n",
    "    :param float dropout: dropout rate\n",
    "    :param str typ: The RNN type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, idim, elayers, cdim, hdim, dropout, typ=\"blstm\"):\n",
    "        super(RNN, self).__init__()\n",
    "        bidir = typ[0] == \"b\"\n",
    "        self.nbrnn = (\n",
    "            torch.nn.LSTM(\n",
    "                idim,\n",
    "                cdim,\n",
    "                elayers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "                bidirectional=bidir,\n",
    "            )\n",
    "            if \"lstm\" in typ\n",
    "            else torch.nn.GRU(\n",
    "                idim,\n",
    "                cdim,\n",
    "                elayers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "                bidirectional=bidir,\n",
    "            )\n",
    "        )\n",
    "        if bidir:\n",
    "            self.l_last = torch.nn.Linear(cdim * 2, hdim)\n",
    "        else:\n",
    "            self.l_last = torch.nn.Linear(cdim, hdim)\n",
    "        self.typ = typ\n",
    "\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, xs_pad, ilens, prev_state=None):\n",
    "        \"\"\"RNN forward\n",
    "        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D)\n",
    "        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n",
    "        :param torch.Tensor prev_state: batch of previous RNN states\n",
    "        :return: batch of hidden state sequences (B, Tmax, eprojs)\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        xs_pack = torch.nn.utils.rnn.pack_padded_sequence(xs_pad, ilens, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        self.nbrnn.flatten_parameters()\n",
    "        \n",
    "        if prev_state is not None and self.nbrnn.bidirectional:\n",
    "            # We assume that when previous state is passed,\n",
    "            # it means that we're streaming the input\n",
    "            # and therefore cannot propagate backward BRNN state\n",
    "            # (otherwise it goes in the wrong direction)\n",
    "            prev_state = reset_backward_rnn_state(prev_state)\n",
    "            \n",
    "        ys, states = self.nbrnn(xs_pack, hx=prev_state)\n",
    "        \n",
    "        # ys: utt list of frame x cdim x 2 (2: means bidirectional)\n",
    "        ys_pad, ilens = torch.nn.utils.rnn.pad_packed_sequence(ys, batch_first=True)\n",
    "        \n",
    "        ys_pad = self.l_last(ys_pad)\n",
    "\n",
    "        xs_pad = self.logsoftmax(ys_pad)\n",
    "        return xs_pad, ilens, states  # x: utt list of frame x dim\n",
    "\n",
    "\n",
    "def reset_backward_rnn_state(states):\n",
    "    \"\"\"Sets backward BRNN states to zeroes\n",
    "    Useful in processing of sliding windows over the inputs\n",
    "    \"\"\"\n",
    "    if isinstance(states, (list, tuple)):\n",
    "        for state in states:\n",
    "            state[1::2] = 0.0\n",
    "    else:\n",
    "        states[1::2] = 0.0\n",
    "    return states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (nbrnn): LSTM(30, 512, num_layers=3, batch_first=True, bidirectional=True)\n",
       "  (l_last): Linear(in_features=1024, out_features=41, bias=True)\n",
       "  (logsoftmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = RNN(30, 3, 512, len(phones_list), 0, typ=\"blstm\")\n",
    "net.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctcloss = torch.nn.CTCLoss(blank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#並列で動作させるにはpadding\n",
    "#https://qiita.com/iBotamon/items/acffef7852faadb420fd\n",
    "#https://takoroy-ai.hatenadiary.jp/entry/2018/07/02/224216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_ID = 1\n",
    "EOS_ID = 2\n",
    "PAD_ID = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    fbanks = []\n",
    "    tokens = []\n",
    "    for feat, label in batch:\n",
    "        fbanks.append(feat)\n",
    "        tokens.append(label)\n",
    "    ilens = torch.tensor([x.shape[0] for x in fbanks])\n",
    "    olens = torch.tensor([x.shape[0] for x in tokens])\n",
    "    return pad_sequence(fbanks, batch_first=True), ilens, pad_sequence(tokens, batch_first=True), olens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(\n",
    "                       dataset,\n",
    "                       batch_size=32,\n",
    "                       shuffle=True,\n",
    "                       collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_ids = [0]\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    net= net.to(device)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "net = net.cuda()\n",
    "#net = torch.nn.DataParallel(net, device_ids=gpu_ids).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_model = \"model/exp_0717\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習ループ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "epocs = 50\n",
    "epoc_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if epoc_start > 0:\n",
    "    net.load_state_dict(torch.load(f'model/exp_0702_net_weight_{epoc_start}', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960192e466964331860e56d7b31f8655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n",
      "/opt/tljh/user/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1162/1615 loss: 0.03723900425008437 step took 2.25383687019348141"
     ]
    }
   ],
   "source": [
    "loss_ave = 0\n",
    "prev_state = None\n",
    "\n",
    "for j in tqdm(range(epoc_start, epocs)):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        a = time.time()\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, inlen, label, oulen = data\n",
    "        output, ilen_y, prev_state = net(inputs.double().cuda(), inlen, prev_state=None)\n",
    "        net.zero_grad()\n",
    "        # loss = ctcloss(output.cpu().transpose(0,1), torch.tensor(label), inlen, oulen)\n",
    "        loss = ctcloss(output.cpu().transpose(0,1), torch.tensor(label), inlen, oulen) / len(output)\n",
    "        loss_ave += loss\n",
    "        # loss.backward(retain_graph=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(net.parameters(), 400)\n",
    "        optimizer.step()\n",
    "        b = time.time()\n",
    "        print (\"\\r{}/{} loss: {} step took {}\".format(i+1,len(trainloader),loss,b-a),end='')\n",
    "        #print (\"output:\\n{} \\n labels:\\n{}\".format(output,label))\n",
    "    torch.save(net.state_dict(), exp_model + \"_net_weight_\" + str(j))\n",
    "    print(\"\\repoc : {}    average loss : {}\".format(j, loss_ave/len(trainloader)))\n",
    "    loss_ave = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
